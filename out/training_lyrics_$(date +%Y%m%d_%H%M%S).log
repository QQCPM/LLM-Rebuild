Overriding: dataset = lyrics
Overriding: n_layer = 12
Overriding: n_head = 6
Overriding: n_embd = 384
Overriding: batch_size = 12
Overriding: block_size = 512
Overriding: max_iters = 15000
Overriding: eval_interval = 1000
Overriding: eval_iters = 50
Overriding: log_interval = 10
Overriding: dropout = 0.1
Overriding: always_save_checkpoint = True
Overriding: init_from = resume
tokens per iteration will be: 245,760
/Users/lending/Library/Python/3.9/lib/python/site-packages/torch/amp/autocast_mode.py:346: UserWarning: In MPS autocast, but the target dtype is not supported. Disabling autocast.
MPS Autocast only supports dtype of torch.bfloat16 and torch.float16 currently.
  warnings.warn(error_message)
found vocab_size = 67 (inside data/lyrics/meta.pkl)
Resuming training from out
number of parameters: 21.27M
/Users/lending/Documents/AI PRJ/LLM-Rebuild/train_gpt2.py:190: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(dtype == 'float16'))
num decayed parameter tensors: 50, with 21,456,000 parameters
num non-decayed parameter tensors: 25, with 9,600 parameters
using fused AdamW: False
step 1000: train loss 0.0479, val loss 0.0512
saving checkpoint to out
iter 1000: loss 0.0550, time 9756.84ms, mfu -100.00%
iter 1010: loss 0.0641, time 5709.86ms, mfu 2.15%
iter 1020: loss 0.0526, time 5715.91ms, mfu 2.15%
iter 1030: loss 0.0636, time 5713.61ms, mfu 2.15%
iter 1040: loss 0.0525, time 5683.37ms, mfu 2.15%
iter 1050: loss 0.1847, time 5673.91ms, mfu 2.15%
iter 1060: loss 0.0761, time 5662.89ms, mfu 2.15%
iter 1070: loss 0.0596, time 5650.16ms, mfu 2.16%
iter 1080: loss 0.0615, time 5660.33ms, mfu 2.16%
iter 1090: loss 0.0634, time 5711.10ms, mfu 2.16%
iter 1100: loss 0.0553, time 5603.26ms, mfu 2.16%
iter 1110: loss 0.0463, time 5597.95ms, mfu 2.16%
iter 1120: loss 0.0877, time 5602.56ms, mfu 2.17%
iter 1130: loss 0.0712, time 5596.70ms, mfu 2.17%
iter 1140: loss 0.0534, time 5607.02ms, mfu 2.17%
iter 1150: loss 0.0497, time 5648.66ms, mfu 2.17%
iter 1160: loss 0.0597, time 5675.78ms, mfu 2.17%
iter 1170: loss 0.0566, time 5661.65ms, mfu 2.17%
iter 1180: loss 0.0513, time 5652.98ms, mfu 2.17%
iter 1190: loss 0.0525, time 5650.96ms, mfu 2.17%
iter 1200: loss 0.0480, time 5655.48ms, mfu 2.17%
iter 1210: loss 0.1069, time 5669.03ms, mfu 2.17%
iter 1220: loss 0.0643, time 5695.23ms, mfu 2.17%
iter 1230: loss 0.0542, time 5625.37ms, mfu 2.17%
iter 1240: loss 0.0516, time 5635.58ms, mfu 2.17%
iter 1250: loss 0.0460, time 5733.18ms, mfu 2.17%
iter 1260: loss 0.0495, time 5712.03ms, mfu 2.17%
iter 1270: loss 0.0544, time 5667.55ms, mfu 2.17%
iter 1280: loss 0.0623, time 5638.29ms, mfu 2.17%
iter 1290: loss 0.0629, time 5631.76ms, mfu 2.17%
iter 1300: loss 0.0499, time 5646.83ms, mfu 2.17%
iter 1310: loss 0.0633, time 5640.21ms, mfu 2.17%
iter 1320: loss 0.0446, time 5652.63ms, mfu 2.17%
iter 1330: loss 0.0392, time 5629.71ms, mfu 2.17%
iter 1340: loss 0.0551, time 5643.75ms, mfu 2.17%
iter 1350: loss 0.0668, time 5635.44ms, mfu 2.17%
iter 1360: loss 0.0516, time 5679.03ms, mfu 2.17%
iter 1370: loss 0.0388, time 5622.70ms, mfu 2.17%
iter 1380: loss 0.0506, time 5635.84ms, mfu 2.17%
iter 1390: loss 0.0477, time 5625.28ms, mfu 2.17%
iter 1400: loss 0.0435, time 5663.20ms, mfu 2.17%
iter 1410: loss 0.1213, time 5623.89ms, mfu 2.18%
iter 1420: loss 0.0476, time 5645.85ms, mfu 2.18%
iter 1430: loss 0.0470, time 5635.32ms, mfu 2.18%
iter 1440: loss 0.0435, time 5674.60ms, mfu 2.17%
iter 1450: loss 0.0525, time 5633.41ms, mfu 2.18%
iter 1460: loss 0.0421, time 5708.23ms, mfu 2.17%
iter 1470: loss 0.0534, time 5627.30ms, mfu 2.17%
iter 1480: loss 0.0535, time 5622.47ms, mfu 2.17%
iter 1490: loss 0.0451, time 5627.44ms, mfu 2.18%
iter 1500: loss 0.0437, time 5653.02ms, mfu 2.18%
iter 1510: loss 0.0410, time 5656.46ms, mfu 2.17%
iter 1520: loss 0.0432, time 5673.11ms, mfu 2.17%
iter 1530: loss 0.0483, time 5662.25ms, mfu 2.17%
